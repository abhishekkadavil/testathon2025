name: AI Performance Testing

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  ai-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install flask requests openai pytest

      - name: Setup Java & Maven
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: "temurin"
      - name: Install HyperExecute CLI
        run: |
          curl -LO https://downloads.lambdatest.com/hyperexecute/linux/hyperexecute
          chmod +x hyperexecute
          mv hyperexecute /usr/local/bin/hyperexecute

      - name: Provide HyperExecute Credentials
        env:
          LT_USERNAME: ${{ secrets.LT_USERNAME }}
          LT_ACCESS_KEY: ${{ secrets.LT_ACCESS_KEY }}
        run: |
          echo "Configured LT Credentials"

      - name: Print hyperexecute.yaml with line numbers
        run: |
          nl -ba hyperexecute.yaml

      # üü¶ Trigger HyperExecute
      - name: Run Gatling on HyperExecute
        env:
          LT_USERNAME: ${{ secrets.LT_USERNAME }}
          LT_ACCESS_KEY: ${{ secrets.LT_ACCESS_KEY }}
        run: |
          set -e
          hyperexecute --config hyperexecute.yaml
          status=$?
          echo "Execution Status: $status"

          if [ $status -ne 0 ]; then
            echo "‚ùå Performance test failed ‚Äî blocking merge"
            exit 1
          else
            echo "‚úÖ Performance test passed"
          fi

      # - name: Generate new test cases (OpenRouter)
      #   env:
      #     OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      #     OPENROUTER_MODEL: ${{ vars.OPENROUTER_MODEL }}
      #     GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      #     GITHUB_REPOSITORY: ${{ github.repository }}
      #     PR_NUMBER: ${{ github.event.pull_request.number }}
      #   run: python tests/generate_tests.py || echo "‚ö†Ô∏è Skipping AI test generation due to rate limit"

      # - name: Start Flask app in background
      #   run: |
      #     echo "üöÄ Starting Flask app..."
      #     nohup python app.py > flask.log 2>&1 &
      #     echo "Waiting for Flask to start..."
      #     for i in {1..20}; do
      #       if curl -s http://127.0.0.1:5000/employee >/dev/null; then
      #         echo "‚úÖ Flask server is up"
      #         break
      #       fi
      #       echo "Waiting..."
      #       sleep 1
      #     done


      # - name: Run Artillery performance tests
      #   run: |
      #     artillery run tests/perf_test.yml -o artillery-report.json
      #     artillery report artillery-report.json

      # - name: Check performance thresholds
      #   run: |
      #     node tests/check_thresholds.js

      # - name: Generate Artillery HTML report
      #   run: |
      #     artillery report artillery-report.json

      # - name: Run all performance tests including generated
      #   run: |
      #     python tests/perf_test.py > results_perf_test.txt
      #     # Run all tests in tests/generated if present
      #     if [ -d "tests/generated" ]; then
      #       for test in tests/generated/*.py; do
      #         python "$test" >> results_perf_test.txt
      #       done
      #     fi

      # - name: Upload test results
      #   if: always()
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: generated-tests-results
      #     path: |
      #       tests/generated/perf_auto.py
      #       results_perf_test.txt
      #       perf_results.json
      #       artillery-report.json
      #       artillery-report.json.html

      - name: Upload HyperExecute Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: hyperexecute-artifacts
          path: "./*"

      # - name: AI Performance Review
      #   env:
      #     OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      #     GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      #     GITHUB_REPOSITORY: ${{ github.repository }}
      #     PR_NUMBER: ${{ github.event.pull_request.number }}
      #   run: python tests/perf_review.py || echo "‚ö†Ô∏è Skipping AI performance review due to rate limit"

      # - name: Force-fail for validation (quick, temporary)
      #   run: |
      #     echo "Deliberate failure to validate branch protection"
      #     exit 1
